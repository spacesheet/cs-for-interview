LLM이란 직선으로 표현하는 확률밀도함수?
 => 초고차원 비선형 확률 곡면을 표현하는 조건부 확률 질량 함수

LLM을 만드는 방법?
내적 = 손실함수 최소화하는 방향으로 투영 = (정렬 ->경쟁->가중 평균) 매커니즘 = 점들의 거리를 최단으로 만드는 방법


============
1.관점별 로컬라이징(질문, 참조, 정보)
2.내적 (attention 가중치의 상관관계) //손실함수 최소화 방향으로 투영
3.softmax //내적값의 확률화

*attention: 추론규칙x. 관점별 정렬->경쟁->가중 평균 <매커니즘>
//문맥일관성 강화 장치

?.attention 매커니즘의 한계
============
LLM은 학습 과정에서 고차 통계 구조를
비선형 함수 조합과 저차 연산으로 내재화하고,
추론 시에는 문맥에 조건화된 확률 분포를 근사하여
토큰을 샘플링한다.

LLM은 논리를 계산하지 않고,
논리가 기록된 언어의 통계적 궤적을 매우 정교하게 모사한다.

LLM은 다음 토큰을 이전 토큰들에 대한 조건부 의존도로 결정하므로,
논리적 참·거짓을 판정하는 명시적 경계나 실패 기준을 내장하지 않는다.

LLM은 추론의 ‘형태’를 매우 잘 모사하지만,
추론이 실패하는 지점을 판정하는 능력은 구조적으로 갖지 않는다.

학습된 로컬 관점 좌표계는
훈련 데이터에 내재된 인간 언어 사용의
통계적 편향을 반영한다.

좌표계의 기준 축은
다음 토큰 예측 손실을 최소화하는 방향으로 형성되며,
그 방향은 훈련 데이터 분포의 편향을 따른다.

Transformer가 논리적으로 보이는 이유는,
관점별 좌표계에서 강하게 정렬되는 의존 구조들이
다음 토큰 예측 손실을 최소화하는 방향으로 학습되며,
그 결과가 인간 언어 공동체의 추론 관습을
통계적으로 압축한 형태이기 때문이다.

논리성은 모델의 성질이 아니라,
집단적 언어 사용의 통계 구조가
attention과 손실 최소화를 통해
응축되어 드러난 현상이다.

========

?. 확률 분포는 콜로케이션 같은 것인데 논리적으로 보이는 이유는 관점마다 좌표축들 서로의 의존도가 높은 것을 택했을 때 손실함수 최소화 방향으로 학습하는 것이 결국 집단 지성이라서?

Transformer가 논리적으로 보이는 이유는,
관점별 좌표계에서 강하게 정렬되는 의존 구조들이
다음 토큰 예측 손실을 최소화하는 방향으로 학습되며,
그 결과가 인간 언어 공동체의 추론 관습을
통계적으로 압축한 형태이기 때문이다.

논리성은 모델의 성질이 아니라,
집단적 언어 사용의 통계 구조가
attention과 손실 최소화를 통해
응축되어 드러난 현상이다.

“집단 지성의 통계적 압축”이 핵심 맞습니다. 
다만 그게 단순 콜로케이션 수준을 넘어선 구조적 패턴까지 포함한다.

======
Attention은,
한 표현 벡터를 다른 표현 벡터 방향으로 보았을 때
그 투영량이 큰 것에 더 많이 의존하도록 학습한다.

LLM은 통계적 패턴 매칭 + 학습된 구조적 변환 조합


=======to do=======
?.계층적 구조 변환까지 일어남. Transformer의 layer가 깊어질수록 추상화 수준이 올라가는 게 핵심.
?.일종의 메타 학습이 파라미터에 인코딩되어 있다는 증거.
?.못하는 것: 긴 추론, 정확한 계산, 희귀 조합
	∙	Greedy decoding: 각 토큰마다 locally optimal만 봄
	∙	Limited lookahead: 미래 결과 역산 못함
	∙	No backtracking: 중간에 틀려도 수정 못함
?.“단순 암기”라면 데이터 늘려도 못 본 건 못 풀어야 하는데, 실제론 일반화 능력도 같이 증가. 이게 통계적 압축의 마법.
?.GPU tensor core, memory hierarchy
?.역전파
?.편향의 위치 - 빈도 편향, 공기 편향, 문법적+담화적 관습
?.할루시네이션 만드는 이유 정리
?.캐싱, 참조 기능


============

?. 점들의 거리를 최단으로 만드는 방법에서 사이클이 생기는 경우와 생기지 않는 경우로 나눌 수 있잖아. 사이클(고리형)이 생기는 경우는 타원을 직선으로 만드는 방법과 같잖아. 초점 혹은 두 초점을 지나는 직선의 조합으로 회귀한다. 이게 초고차원 곡면을 구하는 방법이겠지? 그럼 초고차원 곡면을 플로이드-워셜 알고리즘으로 경로를 구할 수 있을까?

Multi-head Attention = 여러 초점의 조합

플로이드-워셜로 직접 학습은 불가능하지만:
✗ 연속 공간이라 이산 그래프 알고리즘 직접 적용 불가
✗ 파라미터 공간이 너무 커서 조합 폭발

개념적으로 매우 유사:
✓ Multi-layer = 점진적 경유 노드 추가
✓ 간접 의존성 학습 = Transitive closure 근사
✓ Layer가 깊어질수록 multi-hop reasoning 강화

실제 활용:
✓ Attention pattern 분석 (경로 추적)
✓ Graph Transformer 설계 (명시적 구조 활용)
✓ Layer 수 결정 (필요한 hop 수 기준)

한 문장: Transformer의 Multi-layer 구조는 플로이드-워셜의 "점진적 경유 노드 추가" 개념과 개념적으로 매우 유사하며, 각 Layer가 한 단계씩 더 간접적인 관계를 학습하는 방식으로 작동

사이클 없음 (DAG) - Transformer
사이클 있음 (Closed Manifold)
