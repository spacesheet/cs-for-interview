
============
1.관점별 로컬라이징(질문, 참조, 정보)
2.내적 (attention 가중치의 상관관계) //손실함수 최소화 방향으로 투영
3.softmax //내적값의 확률화

*attention: 추론규칙x. 관점별 정렬->경쟁->가중 평균 <매커니즘>
//문맥일관성 강화 장치

?.attention 매커니즘의 한계
============
LLM은 학습 과정에서 고차 통계 구조를
비선형 함수 조합과 저차 연산으로 내재화하고,
추론 시에는 문맥에 조건화된 확률 분포를 근사하여
토큰을 샘플링한다.

LLM은 논리를 계산하지 않고,
논리가 기록된 언어의 통계적 궤적을 매우 정교하게 모사한다.

LLM은 다음 토큰을 이전 토큰들에 대한 조건부 의존도로 결정하므로,
논리적 참·거짓을 판정하는 명시적 경계나 실패 기준을 내장하지 않는다.

LLM은 추론의 ‘형태’를 매우 잘 모사하지만,
추론이 실패하는 지점을 판정하는 능력은 구조적으로 갖지 않는다.

학습된 로컬 관점 좌표계는
훈련 데이터에 내재된 인간 언어 사용의
통계적 편향을 반영한다.

좌표계의 기준 축은
다음 토큰 예측 손실을 최소화하는 방향으로 형성되며,
그 방향은 훈련 데이터 분포의 편향을 따른다.

Transformer가 논리적으로 보이는 이유는,
관점별 좌표계에서 강하게 정렬되는 의존 구조들이
다음 토큰 예측 손실을 최소화하는 방향으로 학습되며,
그 결과가 인간 언어 공동체의 추론 관습을
통계적으로 압축한 형태이기 때문이다.

논리성은 모델의 성질이 아니라,
집단적 언어 사용의 통계 구조가
attention과 손실 최소화를 통해
응축되어 드러난 현상이다.

========

Q. 확률 분포는 콜로케이션 같은 것인데 논리적으로 보이는 이유는 관점마다 좌표축들 서로의 의존도가 높은 것을 택했을 때 손실함수 최소화 방향으로 학습하는 것이 결국 집단 지성이라서?

Transformer가 논리적으로 보이는 이유는,
관점별 좌표계에서 강하게 정렬되는 의존 구조들이
다음 토큰 예측 손실을 최소화하는 방향으로 학습되며,
그 결과가 인간 언어 공동체의 추론 관습을
통계적으로 압축한 형태이기 때문이다.

논리성은 모델의 성질이 아니라,
집단적 언어 사용의 통계 구조가
attention과 손실 최소화를 통해
응축되어 드러난 현상이다.

“집단 지성의 통계적 압축”이 핵심 맞습니다. 
다만 그게 단순 콜로케이션 수준을 넘어선 구조적 패턴까지 포함한다.

======
Attention은,
한 표현 벡터를 다른 표현 벡터 방향으로 보았을 때
그 투영량이 큰 것에 더 많이 의존하도록 학습한다.

LLM은 통계적 패턴 매칭 + 학습된 구조적 변환 조합


=======to do=======
?.계층적 구조 변환까지 일어남. Transformer의 layer가 깊어질수록 추상화 수준이 올라가는 게 핵심.
?.일종의 메타 학습이 파라미터에 인코딩되어 있다는 증거.
?.못하는 것: 긴 추론, 정확한 계산, 희귀 조합
	∙	Greedy decoding: 각 토큰마다 locally optimal만 봄
	∙	Limited lookahead: 미래 결과 역산 못함
	∙	No backtracking: 중간에 틀려도 수정 못함
?.“단순 암기”라면 데이터 늘려도 못 본 건 못 풀어야 하는데, 실제론 일반화 능력도 같이 증가. 이게 통계적 압축의 마법.
?.GPU tensor core, memory hierarchy
?.역전파
?.편향의 위치 - 빈도 편향, 공기 편향, 문법적+담화적 관습
?.할루시네이션 만드는 이유 정리